{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#from string import punctuation\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1= input(\"Enter the document: \")\n",
    "\n",
    "document1 =Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning.In its application across business problems, machine learning is also referred to as predictive analytics.\n",
    "\n",
    "#document2 = \"\"\"Our Father who art in heaven, hallowed be thy name. Thy kingdom come. Thy will be done, on earth as it is in heaven. Give us this day our daily bread; and forgive us our trespasses, as we forgive those who trespass against us; and lead us not into temptation, but deliver us from evil\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readingTime(docs):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc=nlp(docs)\n",
    "    words_tokens =  [ t.text for t in doc]\n",
    "   # print(len(words_tokens))\n",
    "    time  = len(words_tokens)/100\n",
    "    return '{} mins'.format(round(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summarizer(D):\n",
    "   # raw_text = raw_docx\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    docx = nlp(D)\n",
    "    stopwords = list(STOP_WORDS)\n",
    "    # Build Word Frequency\n",
    "    # word.text is tokenization in spacy\n",
    "    word_frequencies = {}  \n",
    "    for word in docx:  \n",
    "        if word.text not in stopwords:\n",
    "            if word.text not in word_frequencies.keys():\n",
    "                word_frequencies[word.text] = 1\n",
    "            else:\n",
    "                word_frequencies[word.text] += 1\n",
    "\n",
    "\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "    # Sentence Tokens\n",
    "    sentence_list = [ sentence for sentence in docx.sents ]\n",
    "\n",
    "    # Calculate Sentence Score and Ranking\n",
    "    sentence_scores = {}  \n",
    "    for sent in sentence_list:  \n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if len(sent.text.split(' ')) < 20:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "\n",
    "    # Find N Largest\n",
    "    summary_sentences = nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "    final_sentences = [ w.text for w in summary_sentences ]\n",
    "    summary = ' '.join(final_sentences)\n",
    "    print(\"Original Document : \\n\")\n",
    "    print(D,\"\\n\")\n",
    "    print(\"Total Length:\",len(D))\n",
    "    print(\"\\n\")\n",
    "   # readingTime(raw_docx)\n",
    "    print(\"Reading time: \",readingTime(D))\n",
    "    \n",
    "    \n",
    "    print('\\n =========================================================\\n Summarized Document\\n')\n",
    "    print(summary,\"\\n\")\n",
    "    print(\"Total Length:\",len(summary))\n",
    "    print(\"\\n\")\n",
    "   # readingTime(summary)\n",
    "    print(\"Reading time: \",readingTime(summary))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summarizer(document1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
